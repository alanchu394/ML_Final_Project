{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eng_to_ipa as ipa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import  Sequential, Model\n",
    "from tensorflow.keras.layers import Layer, Concatenate, Input, Masking, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data (data file already created in data_import.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('words.txt','r')\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "file = open('data.csv','w')\n",
    "\n",
    "file.write('word,pronunciation\\n')\n",
    "for word in lines:\n",
    "    if ipa.isin_cmu(word):\n",
    "        line = word.strip('\\n') + ',' + ipa.convert(word) + '\\n'\n",
    "        file.write(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of initial data: 40315\n",
      "Train size: 32253\n",
      "Val size: 4031\n",
      "Test size: 4031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv ('data.csv',delimiter=',')\n",
    "\n",
    "df.word = df.word.astype(str) \n",
    "df.pronunciation = df.pronunciation.astype(str) \n",
    "\n",
    "#df.applymap(lambda s: s.replace(s,' '.join(str(s))))\n",
    "df['word'] = df['word'].str.replace('',' ')\n",
    "df['pronunciation'] = df['pronunciation'].str.replace('',' ')\n",
    "\n",
    "\n",
    "\n",
    "# 10% for val, 10% for test, 70% for train\n",
    "val_size = int(df.shape[0] * 0.1)\n",
    "test_size = int(df.shape[0] * 0.1)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1)\n",
    "# Split df to test/val/train\n",
    "test_df = df[:test_size]\n",
    "val_df = df[test_size:test_size+val_size]\n",
    "train_df = df[test_size+val_size:]\n",
    "\n",
    "\n",
    "train_words, train_pronounciations = list(train_df.word), list(train_df.pronunciation)\n",
    "val_words, val_pronounciations     = list(val_df.word), list(val_df.pronunciation)\n",
    "test_words, test_prounciations   = list(test_df.word), list(test_df.pronunciation)\n",
    "\n",
    "\n",
    "# Check that idces do not overlap\n",
    "assert set(train_df.index).intersection(set(val_df.index)) == set({})\n",
    "assert set(test_df.index).intersection(set(train_df.index)) == set({})\n",
    "assert set(val_df.index).intersection(set(test_df.index)) == set({})\n",
    "# Check that all idces are present\n",
    "assert df.shape[0] == len(train_pronounciations) + len(val_pronounciations) + len(test_prounciations)\n",
    "\n",
    "# Sizes\n",
    "print(\n",
    "    f\"Size of initial data: {df.shape[0]}\\n\"\n",
    "    f\"Train size: {len(train_pronounciations)}\\n\"\n",
    "    f\"Val size: {len(val_pronounciations)}\\n\"\n",
    "    f\"Test size: {len(test_prounciations)}\\n\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_words, train_pronounciations))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((val_words, val_pronounciations))\n",
    "\n",
    "def str_split(e, g):\n",
    "    e = tf.strings.split(e)\n",
    "    return e, g\n",
    " \n",
    "train_data = train_data.map(str_split)\n",
    "valid_data = valid_data.map(str_split)\n",
    "       \n",
    "embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\")    \n",
    "    \n",
    "def embed_english(x, y):\n",
    "    return embed(x), y\n",
    " \n",
    "train_data = train_data.map(embed_english)\n",
    "valid_data = valid_data.map(embed_english)\n",
    "\n",
    "\n",
    "def remove_long_sentence(e, g):\n",
    "    return tf.shape(e)[0] <= 13\n",
    " \n",
    "train_data = train_data.filter(remove_long_sentence)\n",
    "valid_data = valid_data.filter(remove_long_sentence)\n",
    "\n",
    "\n",
    "def pad_english(e, g):\n",
    "    return tf.pad(e, paddings = [[13-tf.shape(e)[0],0], [0,0]], mode='CONSTANT', constant_values=0), g\n",
    " \n",
    "train_data = train_data.map(pad_english)\n",
    "valid_data = valid_data.map(pad_english)\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(Layer):\n",
    " \n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.embed = tf.Variable(initial_value=tf.zeros(shape=(1,128)), trainable=True, dtype='float32')\n",
    "         \n",
    "    def call(self, inputs):\n",
    "        x = tf.tile(self.embed, [tf.shape(inputs)[0], 1])\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        return tf.concat([inputs, x], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 13, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 14, 128])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer = CustomLayer()\n",
    "e, g = next(iter(train_data.take(1)))\n",
    "print(e.shape)\n",
    "o = custom_layer(e)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(batch_shape = (None, 13, 128), name='input')\n",
    "x = CustomLayer(name='custom_layer')(inputs)\n",
    "x = Masking(mask_value=0, name='masking_layer')(x)\n",
    "x, h, c = LSTM(units=512, return_state=True, name='lstm')(x)\n",
    "encoder_model = Model(inputs = inputs, outputs = [h, c], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(train_words)\n",
    "#pronounciation_tokenizer = Tokenizer(train_pronounciations)\n",
    "print(word_tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling layer \"embedding_layer\" (type Embedding).\n\nCast string to int32 is not supported [Op:Cast]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(16,), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m e, g_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_data\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     18\u001b[0m h, c \u001b[38;5;241m=\u001b[39m encoder_model(e)\n\u001b[1;32m---> 19\u001b[0m g_out, h, c \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(g_out\u001b[38;5;241m.\u001b[39mshape, h\u001b[38;5;241m.\u001b[39mshape, c\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36mDecoder.call\u001b[1;34m(self, inputs, hidden_state, cell_state)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, cell_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     x, hidden_state, cell_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, initial_state \u001b[38;5;241m=\u001b[39m [hidden_state, cell_state]) \\\n\u001b[0;32m     12\u001b[0m                                                  \u001b[38;5;28;01mif\u001b[39;00m hidden_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cell_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x)\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Exception encountered when calling layer \"embedding_layer\" (type Embedding).\n\nCast string to int32 is not supported [Op:Cast]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(16,), dtype=string)"
     ]
    }
   ],
   "source": [
    "class Decoder(Model):\n",
    "     \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.embed = Embedding(input_dim=len(tokenizer.index_word)+1, output_dim=128, mask_zero=True, name='embedding_layer')\n",
    "        self.lstm = LSTM(units = 512, return_state = True, return_sequences = True, name='lstm_layer')\n",
    "        self.dense = Dense(len(tokenizer.index_word)+1, name='dense_layer')\n",
    "         \n",
    "    def call(self, inputs, hidden_state = None, cell_state = None):\n",
    "        x = self.embed(inputs)\n",
    "        x, hidden_state, cell_state = self.lstm(x, initial_state = [hidden_state, cell_state]) \\\n",
    "                                                     if hidden_state is not None and cell_state is not None else self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x, hidden_state, cell_state\n",
    " \n",
    "decoder_model = Decoder(name='decoder')\n",
    "e, g_in = next(iter(train_data.take(1)))\n",
    "h, c = encoder_model(e)\n",
    "g_out, h, c = decoder_model(g_in, h, c)\n",
    " \n",
    "print(g_out.shape, h.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "#word_tokenizer = Tokenizer(train_words)\n",
    "#pronounciation_tokenizer = Tokenizer(train_pronounciations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Embedding(len(train_words), 512, input_length=20, mask_zero=True))\n",
    "#model.add(LSTM(512))\n",
    "#model.add(RepeatVector(20))\n",
    "#model.add(LSTM(512, return_sequences=True))\n",
    "#model.add(Dense(len(train_pronounciations), activation='softmax'))\n",
    " \n",
    "#rms = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "#model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a251302415591a00991e803008012c897957666279c59a7b2e49cbcc4b50ee2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
